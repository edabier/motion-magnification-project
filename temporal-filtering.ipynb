{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.fftpack\n",
    "import scipy.fft as fft\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a temporal filtering to an input video\n",
    "The following function applies a temporal filtering to a video. this filtering can be a `lowpass`, `highpass`, or a `bandpass` filter, and we need to provide the low and high cutting frequencies of these filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temporal_filter(input_video, output_dir, format, low_freq, high_freq, mode):\n",
    "    # Create results directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Processing {input_video} | FPS: {fps}, Frames: {frame_count}, Resolution: {width}x{height}\")\n",
    "\n",
    "    # Create a memory-mapped file to store frames\n",
    "    mmap_file = \"video_frames.dat\"\n",
    "    frames = np.memmap(mmap_file, dtype=np.float16, mode=\"w+\", shape=(frame_count, height, width, 3))\n",
    "\n",
    "    for i in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames[i] = frame.astype(np.float16)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # FFT + Filtering\n",
    "    print(\"Applying FFT (rFFT)...\")\n",
    "    frames_fft = scipy.fft.rfft(frames, axis=0) \n",
    "    freqs = scipy.fft.rfftfreq(frame_count, d=1/fps)\n",
    "\n",
    "    # Creating filters\n",
    "    print(\"Filtering...\")\n",
    "    filtered_fft = np.zeros_like(frames_fft)\n",
    "    lowpass_mask = (freqs <= low_freq)\n",
    "    highpass_mask = (freqs >= high_freq)\n",
    "\n",
    "    # Apply filter: Zero out frequencies outside the selected range\n",
    "    filtered_fft[:] = 0\n",
    "    if mode == \"lowpass\":\n",
    "        np.multiply(frames_fft, lowpass_mask[:, None, None, None], out=filtered_fft)\n",
    "    elif mode == \"highpass\":\n",
    "        np.multiply(frames_fft, highpass_mask[:, None, None, None], out=filtered_fft)\n",
    "    elif mode == \"bandpass\":\n",
    "        bandpass_mask = (freqs >= low_freq) & (freqs <= high_freq)\n",
    "        np.multiply(frames_fft, bandpass_mask[:, None, None, None], out=filtered_fft)\n",
    "\n",
    "    # Inverse FFT to reconstruct filtered motion\n",
    "    print(\"Applying Inverse FFT...\")\n",
    "    filtered_frames = np.real(scipy.fft.irfft(filtered_fft, axis=0)) \n",
    "\n",
    "    # Normalize back to 8-bit\n",
    "    filtered_frames = np.clip(filtered_frames, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Save filtered video\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(input_video))[0]}_filtered{format}\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    print(f\"Saving filtered video to {output_path}\")\n",
    "    for frame in filtered_frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "    print(\"Processing complete!\")  \n",
    "\n",
    "    # Clean up memory-mapped file\n",
    "    del frames\n",
    "    os.remove(mmap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {\"sea_view\": \".mov\", \"grass\": \".mp4\", \"custom\": \".mp4\"}\n",
    "amps = [\"10\", \"20\", \"50\", \"100\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "seq = \"sea_view\" # \"grass\" \"custom\"\n",
    "output_dir = \"filtered_results\"\n",
    "video_format = \".mp4\"\n",
    "\n",
    "# Filter settings\n",
    "low_pass = 2\n",
    "high_pass = 100\n",
    "\n",
    "for i in range(4):\n",
    "    video_name = \"res-videos\" + \"/\" + seq + \"/\" + seq + f\"_amp{amps[i]+sequences[seq]}\"\n",
    "    apply_temporal_filter(video_name, output_dir, video_format, low_pass, high_pass, \"lowpass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing SSIM\n",
    "We want to evaluate the quality of the magnified, and filtered magnified videos compared to the original ones. The goal is to assess the structural fidelity of the magnified videos to the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_video_ssim(video1_path, video2_path, debug=True):\n",
    "    cap1 = cv2.VideoCapture(video1_path)\n",
    "    cap2 = cv2.VideoCapture(video2_path)\n",
    "    \n",
    "    h1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Comparing videos: {video1_path} and {video2_path}\")\n",
    "        print(f\"1st video shape: {h1, int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))}, 2nd: {h2, int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "\n",
    "    if not cap1.isOpened() or not cap2.isOpened():\n",
    "        print(\"Error: Could not open one or both videos.\")\n",
    "        return None\n",
    "\n",
    "    ssim_scores = []\n",
    "    \n",
    "    print(\"Computing SSIM...\")\n",
    "    while True:\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "\n",
    "        # Break if either video ends\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        \n",
    "        # Rotate the second frame if both videos have rotated dimensions\n",
    "        if h1 != h2:\n",
    "            frame2 = cv2.rotate(frame2, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        # SSIM is computed on luminance space => convert to Grayscale\n",
    "        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "        score, _ = ssim(gray1, gray2, full=True)\n",
    "        ssim_scores.append(score)\n",
    "\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    \n",
    "    print(f\"Finished computing SSIM for {video2_path}\")\n",
    "\n",
    "    # Average SSIMs\n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    return avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"custom\" # \"grass\" \"custom\"\n",
    "ssims = []\n",
    "\n",
    "for i in range(4):\n",
    "    video_1 = \"res-videos\" + \"/\" + seq + \"/\" + seq + f\"_amp{amps[i]+sequences[seq]}\"\n",
    "    # video_1 = \"filtered_results\" + \"/\" + seq + f\"_amp{amps[i]}_filtered.mp4\"\n",
    "    video_2 = \"videos/\" + seq + f\"{sequences[seq]}\"\n",
    "    ssims.append(compute_video_ssim(video_1, video_2, debug=True))\n",
    "    \n",
    "print(ssims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python cv",
   "language": "python",
   "name": "python_env_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
